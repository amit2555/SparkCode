#!/usr/bin/python

from datetime import datetime
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from socketIO_client import SocketIO, BaseNamespace
from collections import defaultdict
from automation import tasks
import multiprocessing as mp


def update_dashboard(kwargs):
    socketIO = SocketIO('localhost', 8000)
    for device,content in kwargs.items():
        if content['device_changed']:
	    msg = "Prefixes {} filtered on device {}".format(content['prefixes'],device)
            socketIO.emit('broadcast_event',{'data':msg})
             
def filter_prefixes(parsed_dict):
    jobs = list()
    manager = mp.Manager()
    result_dict = manager.dict()
    
    for device,prefixes in parsed_dict.iteritems():
        job = mp.Process(target=tasks.apply_filter_config,args=(device,prefixes,result_dict))
        job.start()
        jobs.append(job)

    for job in jobs:
        job.join()

    return result_dict

def inconsistency_check(iter):
    to_be_actioned = list()
 
    for record in iter:
        prefix,prefix_data = record # Tuple unpacking

        # Compare only if a particular prefix is learnt from more than one neighbors
        if len(prefix_data) > 1:
            # Convert dictionaries to a set and find common attributes
            bgp_prefix_set = (set(x.items()) for x in prefix_data) # This is a generator.
            common_attributes = dict(reduce(set.intersection,bgp_prefix_set)) # Find common BGP attributes between grouped advertisement.

            attributes_to_check = ['MED','AS_Path','AS_Path_Count','Origin_AS'] 
            inconsistent_attributes = list()
            inconsistency_detected = False

            for attribute in attributes_to_check:
                if attribute not in common_attributes:
	            inconsistent_attributes.append(attribute)
		    inconsistency_detected = True

	    if inconsistency_detected:
                devices = [ (entry['Router_IP'],prefix) for entry in prefix_data ]
                to_be_actioned.extend(devices)

    #print to_be_actioned	#This is a list [(router,prefix),(router2,prefix2)..] as generated by a single core. A similar list is produced by other cores.
   
    if len(to_be_actioned) > 0:
        # Transform to the form: { router:[prefixes],... }
        transformed = defaultdict(list)
        for key, value in to_be_actioned:
            transformed[key].append(value)

        #print transformed
        filtered = filter_prefixes(transformed)
        
	update_dashboard(filtered)


def structure_data(line):
    fields = ['Action',
    'Sequence',
    'Hash',
    'Router_Hash',
    'Router_IP',
    'Base_Attr_Hash',
    'Peer_Hash',
    'Peer_IP',
    'Peer_ASN',
    'Timestamp',
    'Prefix',
    'PrefixLen',
    'isIPv4',
    'Origin',
    'AS_Path',
    'AS_Path_Count',
    'Origin_AS',
    'Next_Hop',
    'MED',
    'Local_Pref',
    'Aggregator',
    'Community_List',
    'Ext_Community_List',
    'Cluster_List',
    'isAtomicAgg',
    'isNextHopIPv4',
    'Originator_Id']

    d = dict()
    for k,v in zip(fields,line.split('\t')):
        d[k] = str(v)
    d['Prefix'] = d['Prefix'] + '/' + d['PrefixLen']
    d.pop('PrefixLen')
    return d

def convert_to_keyvalue(prefix_entry):
    prefix = prefix_entry['Prefix']
    tuple_data = (prefix,prefix_entry)
    return tuple_data
 

sc = SparkContext(appName="BGPInconsistencyCheck")
ssc = StreamingContext(sc,10)

zkQuorum = 'localhost:2181'
topic = 'openbmp.parsed.unicast_prefix'
partition = 2 	#Number of threads to consume data from Kafka 
kafkaStream = KafkaUtils.createStream(ssc, zkQuorum, "spark-streaming-consumer", {topic: partition})

lines = kafkaStream.flatMap(lambda x: x[1].splitlines()).filter(lambda line: line.startswith('add'))

paired_rdd = lines.map(structure_data).map(convert_to_keyvalue)

grouped_rdd = paired_rdd.groupByKey().mapValues(list)

grouped_rdd.foreachRDD(lambda rdd: rdd.foreachPartition(inconsistency_check))

ssc.start()
ssc.awaitTermination()

